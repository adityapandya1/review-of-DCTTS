{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ytig9a191QR9"
   },
   "source": [
    "<h4>Repository- <a>https://github.com/adityapandya1/review-of-DCTTS</a></h4>\n",
    "\n",
    "<h4>Paper- <a>https://arxiv.org/abs/1710.08969</a></h4>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bKyvEwMC1QSC"
   },
   "source": [
    "<h1>Requirements</h1>\n",
    "<br>\n",
    "<span>\n",
    "<b>\n",
    "\n",
    "librosa==0.5.1<br>\n",
    "matplotlib==2.0.2<br>\n",
    "numpy==1.13.3<br>\n",
    "scipy==0.19.1<br>\n",
    "tensorflow==1.4.0<br>\n",
    "tqdm==4.19.2<br>\n",
    "</b>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>DOWNLOAD DATASET</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "aIcFqru31QSF",
    "outputId": "fa7c8ae5-2346-4b24-9098-a719cba4db09"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "tar: Error opening archive: Failed to open 'LJSpeech-1.0.tar.bz2'\n"
     ]
    }
   ],
   "source": [
    "!wget http://data.keithito.com/data/speech/LJSpeech-1.0.tar.bz2\n",
    "!tar xjf LJSpeech-1.0.tar.bz2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>INSTALL ALL REQUIREMENTS</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 781
    },
    "colab_type": "code",
    "id": "K9k5ki8n1QSQ",
    "outputId": "1a8d8ea7-31b8-40fa-ddd1-7c9e102cdd42",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install librosa --user\n",
    "!pip install matplotlib --user\n",
    "!pip install numpy --user\n",
    "!pip install scipy --user\n",
    "!pip install tqdm --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall tensorflow\n",
    "!pip install tensorflow==1.14.0 --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AFvT01a_1QSX"
   },
   "source": [
    "<h1>HYPER PARAMETERS</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DIh3dlM91QSY"
   },
   "outputs": [],
   "source": [
    "data_dir = 'LJSpeech-1.0/' \n",
    "\n",
    "data = 'LJSpeech-1.0/'\n",
    "\n",
    "metafile = 'LJSpeech-1.0/metadata.csv'\n",
    "\n",
    "batch_size = 16 # alias = N\n",
    "\n",
    "warmup_steps = 4000\n",
    "\n",
    "logdir = 'logdir' # log directory\n",
    "\n",
    "logdirmag = 'logdirmag' # log directory\n",
    "\n",
    "logdirmel = 'logdirmel' # log directory\n",
    "\n",
    "sr = 22050 # Sampling Rate\n",
    "\n",
    "n_fft = 2048 # fft points (samples) (Fast Fourier Transform)\n",
    "\n",
    "fd = 1+n_fft//2\n",
    "\n",
    "frame_shift = 0.0125 # seconds\n",
    "\n",
    "frame_length = 0.05 # seconds\n",
    "\n",
    "hop_length = 256 # samples\tThis is dependent on the frame_shift.\n",
    "\n",
    "win_length = 1024 # samples This is dependent on the frame_length.\n",
    "\n",
    "n_mels = 80 # Number of Mel banks to generate\n",
    "\n",
    "sharpening_factor = 1.4 # Exponent for amplifying the predicted magnitude\n",
    "\n",
    "n_iter = 50 # Number of inversion iterations\n",
    "\n",
    "preemphasis = .97 # or None\n",
    "\n",
    "griffin_lim_iters=60\n",
    "\n",
    "power=1.5              # Power to raise magnitudes to prior to Griffin-Lim\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "max_db = 100\n",
    "\n",
    "min_db = -100\n",
    "\n",
    "ref_db = 20\n",
    "\n",
    "max_grad_norm = 100.\n",
    "\n",
    "max_grad_val = 5.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model\n",
    "\n",
    "maxlen = 180 # Maximum number of letters in a sentance = T.\n",
    "\n",
    "Ty = 868 # Max number of timesteps \n",
    "\n",
    "Tyr = Ty//4 # Max number of timesteps \n",
    "\n",
    "e = 128\n",
    "\n",
    "d = 256\n",
    "\n",
    "c = 512\n",
    "\n",
    "lr = 2e-4\n",
    "\n",
    "init_lr=2e-4\n",
    "\n",
    "g=0.2\n",
    "\n",
    "b1 = 0.5\n",
    "\n",
    "b2 = 0.9\n",
    "\n",
    "eps = 1e-6\n",
    "\n",
    "logevery = 200\n",
    "\n",
    "dropout_rate = 0.1\n",
    "\n",
    "masking = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XKMFhYIt1QSf"
   },
   "source": [
    "<h1>DATA PREPROCESSING</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "UoG_P5kq1QSg",
    "outputId": "c34beb2a-d9c0-4f52-d539-813b1731d39a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import tqdm\n",
    "\n",
    "\n",
    "def get_spectrograms(sound_file):\n",
    "    '''Returns normalized log(melspectrogram) and log(magnitude) from `sound_file`.\n",
    "    Args:\n",
    "      sound_file: A string. The full path of a sound file.\n",
    "    Returns:\n",
    "      mel: A 2d array of shape (T, n_mels) <- Transposed\n",
    "      mag: A 2d array of shape (T, 1+n_fft/2) <- Transposed\n",
    "    '''\n",
    "    # Loading sound file\n",
    "    y, sr = librosa.load(sound_file, sr=22050)\n",
    "\n",
    "    # Trimming\n",
    "    y, _ = librosa.effects.trim(y)\n",
    "\n",
    "    # Preemphasis\n",
    "    y = np.append(y[0], y[1:] - preemphasis * y[:-1])\n",
    "\n",
    "    # stft\n",
    "    linear = librosa.stft(y=y,\n",
    "                          n_fft=n_fft,\n",
    "                          hop_length=hop_length,\n",
    "                          win_length=win_length)\n",
    "\n",
    "    # magnitude spectrogram\n",
    "    mag = np.abs(linear)  # (1+n_fft//2, T)\n",
    "\n",
    "    # mel spectrogram\n",
    "    mel_basis = librosa.filters.mel(sr, n_fft, n_mels)  # (n_mels, 1+n_fft//2)\n",
    "    mel = np.dot(mel_basis, mag)  # (n_mels, t)\n",
    "\n",
    "    # Sequence length\n",
    "    done = np.ones_like(mel[0, :]).astype(np.int32)\n",
    "\n",
    "    # to decibel\n",
    "    mel = librosa.amplitude_to_db(mel)\n",
    "    mag = librosa.amplitude_to_db(mag)\n",
    "\n",
    "    # normalize\n",
    "    mel = np.clip((mel - ref_db + max_db) / max_db, 0, 1)\n",
    "    mag = np.clip((mag - ref_db + max_db) / max_db, 0, 1)\n",
    "\n",
    "    # Transpose\n",
    "    mel = mel.T.astype(np.float32)  # (T, n_mels)\n",
    "    mag = mag.T.astype(np.float32)  # (T, 1+n_fft//2)\n",
    "\n",
    "    return mel, done, mag\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    wav_folder = os.path.join(data, 'wavs')\n",
    "    # wav_folder = os.path.join('/data/private/voice/nick', 'Tom')\n",
    "    mel_folder = os.path.join(data, 'mels')\n",
    "    dones_folder = os.path.join(data, 'dones')\n",
    "    mag_folder = os.path.join(data, 'mags')\n",
    "\n",
    "    for folder in (mel_folder, dones_folder, mag_folder):\n",
    "        if not os.path.exists(folder): os.mkdir(folder)\n",
    "\n",
    "    files = glob.glob(os.path.join(wav_folder, \"*\"))\n",
    "    for f in tqdm.tqdm(files):\n",
    "        fname = os.path.basename(f)\n",
    "        mel, dones, mag = get_spectrograms(f)  # (n_mels, T), (1+n_fft/2, T) float32\n",
    "        np.save(os.path.join(mel_folder, fname.replace(\".wav\", \".npy\")), mel)\n",
    "        np.save(os.path.join(dones_folder, fname.replace(\".wav\", \".npy\")), dones)\n",
    "        np.save(os.path.join(mag_folder, fname.replace(\".wav\", \".npy\")), mag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FnL_F3QT1QSn"
   },
   "source": [
    "<h1>AUDIO</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qbGki_qi1QSo"
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.filters\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "#import tensorflow as tf\n",
    "\n",
    "\n",
    "def load_wav(path):\n",
    "    \n",
    "    '''\n",
    "        Load an audio file as a floating point time series.\n",
    "\n",
    "        Audio will be automatically resampled to the given rate (default sr=22050).\n",
    "\n",
    "        To preserve the native sampling rate of the file, use sr=None.\n",
    "\n",
    "        ARGS: File Path\n",
    "\n",
    "        RETURNS: Time-Intensity Representaion Of WaveForm\n",
    "        \n",
    "        '''\n",
    "    \n",
    "    return librosa.core.load(path, sr=sr)[0]\n",
    "\n",
    "\n",
    "def save_wav(wav, path):\n",
    "    \n",
    "    '''\n",
    "        Saves Audio File To The Given Path\n",
    "    \n",
    "        ARGS: Time-Intensity Representaion Of Waveform , Path\n",
    "        \n",
    "        RETURNS: Nothing,Saves the Waveform To The Path\n",
    "    '''\n",
    "    \n",
    "    wav *= 32767 / max(0.01, np.max(np.abs(wav)))\n",
    "    librosa.output.write_wav(path, wav.astype(np.int16), sr)\n",
    "    \n",
    "\n",
    "\n",
    "def preemphasis(x):\n",
    "    \n",
    "    '''\n",
    "        In high speed digital transmission, pre-emphasis is used to improve signal quality at the output of a \n",
    "        data transmission. \n",
    "        In transmitting signals at high data rates, the transmission medium may introduce distortions, \n",
    "        so pre-emphasis is used to distort the transmitted signal to correct for this distortion.\n",
    "    \n",
    "        ARGS: Time-Intensity Representation Of WaveForm\n",
    "    \n",
    "        RETURNS: Time-Intensity Representation Of Waveform With Improved Signal Quality With Reduced Noise\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    return signal.lfilter([1, -preemphasis], [1], x)\n",
    "\n",
    "\n",
    "def inv_preemphasis(x):\n",
    "    \n",
    "    '''\n",
    "        DE-EMPHASIS\n",
    "    \n",
    "        ARGS : Time-Intensity Representation Of WaveForm\n",
    "        \n",
    "        RETURNS: Time-Intensity Representation Of WaveForm With Added Noise\n",
    "    '''\n",
    "    \n",
    "    return signal.lfilter([1], [1, -preemphasis], x)\n",
    "\n",
    "\n",
    "def spectrogram(y):\n",
    "    \n",
    "    \n",
    "    '''\n",
    "        Converts a WaveForm To Spectogram using Short-Time-Fourier-Transform\n",
    "        And Then Converting Amplitude to Decibels For Proper Scaling\n",
    "        \n",
    "        ARGS: Time-Intensity Representation Of WaveForm\n",
    "        \n",
    "        RETURNS: SPECTOGRAM\n",
    "    '''\n",
    "    \n",
    "    D = _stft(preemphasis(y))  #Short-Time-Fourier-Transform\n",
    "    \n",
    "    S = _amp_to_db(np.abs(D)) - ref_db  #Converts Waveform To Spectogram\n",
    "    \n",
    "    return _normalize(S)\n",
    "\n",
    "def save_spec(spectrogram,path):\n",
    "    \n",
    "    '''\n",
    "        Converts The Spectogram To WaveForm And Then Save It To The Given Path\n",
    "\n",
    "        ARGS: Spectogram , Path\n",
    "        \n",
    "        RETURNS: Nothing , Saves Spectogram To Given Path\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    wav = inv_spectrogram(spectrogram)\n",
    "    save_wav(wav,path)\n",
    "\n",
    "def inv_spectrogram(spectrogram):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "        Converts Spectogram To Waveform\n",
    "        \n",
    "        ARGS: Spectogram\n",
    "        \n",
    "        RETURNS: Time-Intensity Representaion Of WaveForm\n",
    "     \n",
    "    '''\n",
    "    S = _db_to_amp(_denormalize(spectrogram) + ref_db)  # Convert back to linear\n",
    "    return inv_preemphasis(_griffin_lim(S ** power))          # Reconstruct phase\n",
    "\n",
    "def inv_spectrogram_tensorflow(spectrogram):\n",
    "    \n",
    "    '''Builds computational graph to convert spectrogram to waveform using TensorFlow.\n",
    "    Unlike inv_spectrogram, this does NOT invert the preemphasis. The caller should call\n",
    "    inv_preemphasis on the output after running the graph.\n",
    "    '''\n",
    "    S = _db_to_amp_tensorflow(_denormalize_tensorflow(spectrogram) + ref_db)\n",
    "    return _griffin_lim_tensorflow(tf.pow(S, power))\n",
    "\n",
    "def _griffin_lim_tensorflow(S):\n",
    "    \n",
    "    '''TensorFlow implementation of Griffin-Lim\n",
    "    Based on https://github.com/Kyubyong/tensorflow-exercises/blob/master/Audio_Processing.ipynb\n",
    "    '''\n",
    "    with tf.variable_scope('griffinlim'):\n",
    "        \n",
    "    # TensorFlow's stft and istft operate on a batch of spectrograms; create batch of size 1\n",
    "        S = tf.expand_dims(S, 0)\n",
    "        S_complex = tf.identity(tf.cast(S, dtype=tf.complex64))\n",
    "        y = _istft_tensorflow(S_complex)\n",
    "        for i in range(griffin_lim_iters):\n",
    "            est = _stft_tensorflow(y)\n",
    "            angles = est / tf.cast(tf.maximum(1e-8, tf.abs(est)), tf.complex64)\n",
    "            y = _istft_tensorflow(S_complex * angles)\n",
    "    return tf.squeeze(y, 0)\n",
    "\n",
    "def _denormalize_tensorflow(S):\n",
    "    \n",
    "    return (tf.clip_by_value(S, 0, 1) * -min_db) + min_db\n",
    "\n",
    "def _db_to_amp_tensorflow(x):\n",
    "    \n",
    "    return tf.pow(tf.ones(tf.shape(x)) * 10.0, x * 0.05)\n",
    "  \n",
    "def _griffin_lim(S):\n",
    "    \n",
    "    ''' To  retrieve  a time-domain signal from its amplitude spectrogram,\n",
    "        the corresponding phase is required. One of the popular phase reconstruction methods\n",
    "        is the Griffin–Lim algorithm (GLA), which is based on the re-dundancy of the \n",
    "        Short-Time Fourier transform\n",
    "\n",
    "        Based on https://github.com/librosa/librosa/issues/434\n",
    "\n",
    "        ARGS: Spectogram\n",
    "\n",
    "        RETURNS: Time-Intensity Representation of WaveForm With Reconstructed Phase\n",
    "    \n",
    "    '''\n",
    "    angles = np.exp(2j * np.pi * np.random.rand(*S.shape))\n",
    "    S_complex = np.abs(S).astype(np.complex)\n",
    "    y = _istft(S_complex * angles)\n",
    "    for i in range(griffin_lim_iters):\n",
    "        angles = np.exp(1j * np.angle(_stft(y)))\n",
    "        y = _istft(S_complex * angles)\n",
    "    return y\n",
    "\n",
    "\n",
    "def _stft(y):\n",
    "    \n",
    "    '''\n",
    "        Implementation Of Short-Time Fourier Transform Usiing Librosa\n",
    "\n",
    "        ARGS: Time-Intensity Representaion Of WaveForm\n",
    "        \n",
    "        RETURNS: Amplitude-Frequency Domain Representation Of A WaveForm With Specific Time Hops\n",
    "\n",
    "    '''\n",
    "    \n",
    "    n_fft, hop_length, win_length = _stft_parameters()\n",
    "    return librosa.stft(y=y, n_fft=n_fft, hop_length=hop_length, win_length=win_length)\n",
    "\n",
    "\n",
    "def _istft(y):\n",
    "    \n",
    "    '''\n",
    "        Inverse-Short-Time Fourier Transform - Converts a Complex Spectogram To A Time Domain Representaion\n",
    "        Of The Waveform\n",
    "    \n",
    "        ARGS: Spectogram\n",
    "        \n",
    "        RETURNS: Time-Intensity Representation Of WaveForm\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    _, hop_length, win_length = _stft_parameters()\n",
    "    return librosa.istft(y, hop_length=hop_length, win_length=win_length)\n",
    "\n",
    "def _istft_tensorflow(stfts):\n",
    "    \n",
    "    n_fft, hop_length, win_length = _stft_parameters()\n",
    "    return tf.contrib.signal.inverse_stft(stfts, win_length, hop_length, n_fft)\n",
    "\n",
    "def _stft_tensorflow(signals):\n",
    "    \n",
    "    n_fft, hop_length, win_length = _stft_parameters()\n",
    "    return tf.contrib.signal.stft(signals, win_length, hop_length, n_fft, pad_end=False)\n",
    "\n",
    "def _stft_parameters():\n",
    "    \n",
    "    \n",
    "    '''\n",
    "        Defining Necessary Parameters to perform Short-Time Fourier Transform\n",
    "\n",
    "        ARGS: None\n",
    "\n",
    "        OUTPUT: Necessary Parameters\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    n_fft = n_fft\n",
    "    hop_length = int(frame_shift * sr)\n",
    "    win_length = int(frame_length  * sr)\n",
    "    hop_length = hop_length\n",
    "    win_length = win_length\n",
    "    return n_fft, hop_length, win_length\n",
    "\n",
    "\n",
    "# Conversions:\n",
    "\n",
    "\n",
    "\n",
    "def _amp_to_db(x):\n",
    "    \n",
    "    '''\n",
    "    Converts Amplitude To Decibel Values\n",
    "    \n",
    "    ARGS: Amplitude - Frequency Representation Of WaveForm\n",
    "    \n",
    "    RETURNS: Decible- Frequency Representaion Of WaveForm\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    return 20 * np.log10(np.maximum(1e-5, x))\n",
    "\n",
    "def _db_to_amp(x):\n",
    "    \n",
    "    '''\n",
    "        Converts a Decibel-Frequency Representation To Amplitude-Frequency Representation Of A WaveForm\n",
    "\n",
    "    \n",
    "        ARGS: Decible - Frequency Representation Of WaveForm\n",
    "\n",
    "        RETURNS: Amplitude- Frequency Representaion Of WaveForm\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    return np.power(10.0, x * 0.05)\n",
    "\n",
    "\n",
    "def _normalize(S):\n",
    "    \n",
    "    '''\n",
    "        Normalizes A Spectogram By Clipping Or Limiting Values In a Spectogram Between 0 and 1\n",
    "\n",
    "        ARGS: Spectogram\n",
    "\n",
    "        RETURNS: Normalized Spectogram\n",
    "\n",
    "    ''' \n",
    "    \n",
    "    return np.clip((S - min_db) / -min_db, 0, 1)\n",
    "\n",
    "def _denormalize(S):\n",
    "    \n",
    "    '''\n",
    "        De-Normalizes or Reverts The Normalized Spectogram Back To Original Representation\n",
    "        \n",
    "        ARGS: Normalized Sepctogram\n",
    "        \n",
    "        RETURNS : De-Normalized Spectogram\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    return (np.clip(S, 0, 1) * -min_db) + min_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "90SEvxjp1QSu"
   },
   "source": [
    "<h1>Modules</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5dSu_GHG1QSw"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "\n",
    "def embedding(inputs, \n",
    "              vocab_size, \n",
    "              num_units, \n",
    "              zero_pad=False, \n",
    "              scale=True,\n",
    "              scope=\"embedding\", \n",
    "              reuse=None):\n",
    "    \n",
    "    '''Embeds a given tensor.\n",
    "    Args:\n",
    "      inputs: A `Tensor` with type `int32` or `int64` containing the ids\n",
    "         to be looked up in `lookup table`.\n",
    "      vocab_size: An int. Vocabulary size.\n",
    "      num_units: An int. Number of embedding hidden units.\n",
    "      zero_pad: A boolean. If True, all the values of the fist row (id 0)\n",
    "        should be constant zeros.\n",
    "      scale: A boolean. If True. the outputs is multiplied by sqrt num_units.\n",
    "      scope: Optional scope for `variable_scope`.\n",
    "      reuse: Boolean, whether to reuse the weights of a previous layer\n",
    "        by the same name.\n",
    "    Returns:\n",
    "      A `Tensor` with one more rank than inputs's. The last dimensionality\n",
    "        should be `num_units`.\n",
    "\n",
    "    For example,\n",
    "\n",
    "    ```\n",
    "    import tensorflow as tf\n",
    "\n",
    "    inputs = tf.to_int32(tf.reshape(tf.range(2*3), (2, 3)))\n",
    "    outputs = embedding(inputs, 6, 2, zero_pad=True)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print sess.run(outputs)\n",
    "    >>\n",
    "    [[[ 0.\t\t\t0.\t\t  ]\n",
    "      [ 0.09754146\t0.67385566]\n",
    "      [ 0.37864095 -0.35689294]]\n",
    "     [[-1.01329422 -1.09939694]\n",
    "      [ 0.7521342\t0.38203377]\n",
    "      [-0.04973143 -0.06210355]]]\n",
    "    ```\n",
    "\n",
    "    ```\n",
    "    import tensorflow as tf\n",
    "\n",
    "    inputs = tf.to_int32(tf.reshape(tf.range(2*3), (2, 3)))\n",
    "    outputs = embedding(inputs, 6, 2, zero_pad=False)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print sess.run(outputs)\n",
    "    >>\n",
    "    [[[-0.19172323 -0.39159766]\n",
    "      [-0.43212751 -0.66207761]\n",
    "      [ 1.03452027 -0.26704335]]\n",
    "     [[-0.11634696 -0.35983452]\n",
    "      [ 0.50208133\t0.53509563]\n",
    "      [ 1.22204471 -0.96587461]]]\t \n",
    "    ```\t   \n",
    "    '''\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        lookup_table = tf.get_variable('lookup_table',\n",
    "                                       dtype=tf.float32,\n",
    "                                       shape=[vocab_size, num_units],\n",
    "                                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "        if zero_pad:\n",
    "            lookup_table = tf.concat((tf.zeros(shape=[1, num_units]),\n",
    "                                      lookup_table[1:, :]), 0)\n",
    "        outputs = tf.nn.embedding_lookup(lookup_table, inputs)\n",
    "\n",
    "        if scale:\n",
    "            outputs = outputs * (num_units ** 0.5) \n",
    "\n",
    "    return outputs\n",
    "\n",
    "def conv1d(inputs, \n",
    "           filters, \n",
    "           size=1, \n",
    "           rate=1, \n",
    "           padding=\"SAME\", \n",
    "           causal=False,\n",
    "           use_bias=False,\n",
    "           scope=\"conv1d\"):\n",
    "    '''\n",
    "    Args:\n",
    "      inputs: A 3-D tensor of [batch, time, depth].\n",
    "      filters: An int. Number of outputs (=activation maps)\n",
    "      size: An int. Filter size.\n",
    "      rate: An int. Dilation rate.\n",
    "      padding: Either `SAME` or `VALID`.\n",
    "      causal: A boolean. If True, zeros of (kernel size - 1) * rate are padded on the left\n",
    "        for causality.\n",
    "      use_bias: A boolean.\n",
    "    \n",
    "    Returns:\n",
    "      A masked tensor of the sampe shape as `tensor`.\n",
    "    '''\n",
    "    \n",
    "    with tf.variable_scope(scope):\n",
    "        if causal:\n",
    "            # pre-padding for causality\n",
    "            pad_len = (size - 1) * rate  # padding size\n",
    "            inputs = tf.pad(inputs, [[0, 0], [pad_len, 0], [0, 0]])\n",
    "            padding = \"VALID\"\n",
    "            \n",
    "        params = {\"inputs\":inputs, \"filters\":filters, \"kernel_size\":size,\n",
    "                \"dilation_rate\":rate, \"padding\":padding, \"activation\":None, \n",
    "                \"use_bias\":use_bias}\n",
    "        print(\"conv1d inputs = {}\".format(inputs))\n",
    "        out = tf.layers.conv1d(**params)\n",
    "        #print(\"THIS FUNCTION IS WORKING FOR SOME PARTS\")\n",
    "    \n",
    "    return out\n",
    "\n",
    "def conv1d_transpose(x,filters,kernel_size,strides):\n",
    "    \n",
    "    x = tf.expand_dims(x,1)\n",
    "    outputs=tf.layers.conv2d_transpose(x,filters,kernel_size,strides=(1,strides),padding='same')\n",
    "    outputs = tf.squeeze(outputs,1)\n",
    "    return outputs\n",
    "\n",
    "def Deconv1D(inputs, channels, kernel_size,dilation,scope=\"deconv1d\"):\n",
    "    \n",
    "    with tf.variable_scope(scope, reuse=False):\n",
    "        outputs = conv1d_transpose(inputs,channels,kernel_size,2)\n",
    "        return outputs\n",
    "\n",
    "def Conv1D(inputs, channels, kernel_size, dilation,causal=True,is_training=True,dropout=0.1, activation=None, scope = \"Conv1D\", reuse=None):\n",
    "    \n",
    "    print(\"CONV1D inputs = {}\".format(inputs))\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        outputs = conv1d(inputs, channels, size=kernel_size, scope=scope, rate=dilation, causal=causal,)\n",
    "        if activation is not None:\n",
    "            outputs=activation(outputs)\n",
    "        return tf.layers.dropout(outputs, rate=dropout,training=is_training)\n",
    "\n",
    "def HConv1D(inputs, channels, kernel_size, dilation, causal=True,is_training=True, activation=None, scope = \"HConv1D\", reuse=None):\n",
    "    \n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        H = Conv1D(inputs, 2*channels, kernel_size, dilation=dilation, causal=causal,is_training=is_training,activation=activation,scope='c1d-H')\n",
    "        H1,H2 = tf.split(H,num_or_size_splits=2,axis=2)\n",
    "        H1 = tf.nn.sigmoid(H1)\n",
    "        return H1 * H2 + inputs * (1.0 - H1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UYhoGMQX1QS4"
   },
   "source": [
    "<h1>TEXT2MEL MODEL</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 610
    },
    "colab_type": "code",
    "id": "fTngM1sV1QS6",
    "outputId": "330eb69f-deea-4f67-942b-b8784419cd87"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0517 15:26:44.037171 12536 deprecation.py:323] From <ipython-input-9-a00be408c0e7>:45: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \n",
      "W0517 15:26:44.245564 12536 deprecation.py:323] From <ipython-input-9-a00be408c0e7>:50: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor 'IteratorGetNext:0' shape=(?, ?) dtype=int32>, <tf.Tensor 'IteratorGetNext:1' shape=(?, ?, ?) dtype=float32>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0517 15:26:56.001794 12536 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0517 15:26:56.085747 12536 deprecation.py:323] From <ipython-input-8-3139ff40e5d6>:113: conv1d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv1D` instead.\n",
      "W0517 15:26:56.096740 12536 deprecation.py:506] From c:\\users\\abhishek\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONV1D inputs = Tensor(\"Text2Mel/TextEnc/embedding/embedding_lookup/Identity:0\", shape=(?, ?, 128), dtype=float32)\n",
      "conv1d inputs = Tensor(\"Text2Mel/TextEnc/embedding/embedding_lookup/Identity:0\", shape=(?, ?, 128), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0517 15:26:57.554459 12536 deprecation.py:323] From <ipython-input-8-3139ff40e5d6>:138: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONV1D inputs = Tensor(\"Text2Mel/TextEnc/c1d-1/dropout/dropout/mul_1:0\", shape=(?, ?, 512), dtype=float32)\n",
      "conv1d inputs = Tensor(\"Text2Mel/TextEnc/c1d-1/dropout/dropout/mul_1:0\", shape=(?, ?, 512), dtype=float32)\n",
      "CONV1D inputs = Tensor(\"Text2Mel/TextEnc/c1d-2/dropout/dropout/mul_1:0\", shape=(?, ?, 512), dtype=float32)\n",
      "conv1d inputs = Tensor(\"Text2Mel/TextEnc/c1d-2/dropout/dropout/mul_1:0\", shape=(?, ?, 512), dtype=float32)\n",
      "CONV1D inputs = Tensor(\"Text2Mel/TextEnc/hc1d-1-0/add:0\", shape=(?, ?, 512), dtype=float32)\n",
      "conv1d inputs = Tensor(\"Text2Mel/TextEnc/hc1d-1-0/add:0\", shape=(?, ?, 512), dtype=float32)\n",
      "CONV1D inputs = Tensor(\"Text2Mel/TextEnc/hc1d-2-0/add:0\", shape=(?, ?, 512), dtype=float32)\n",
      "conv1d inputs = Tensor(\"Text2Mel/TextEnc/hc1d-2-0/add:0\", shape=(?, ?, 512), dtype=float32)\n",
      "CONV1D inputs = Tensor(\"Text2Mel/TextEnc/hc1d-3-0/add:0\", shape=(?, ?, 512), dtype=float32)\n",
      "conv1d inputs = Tensor(\"Text2Mel/TextEnc/hc1d-3-0/add:0\", shape=(?, ?, 512), dtype=float32)\n",
      "CONV1D inputs = Tensor(\"Text2Mel/TextEnc/hc1d-4-0/add:0\", shape=(?, ?, 512), dtype=float32)\n",
      "conv1d inputs = Tensor(\"Text2Mel/TextEnc/hc1d-4-0/add:0\", shape=(?, ?, 512), dtype=float32)\n",
      "CONV1D inputs = Tensor(\"Text2Mel/TextEnc/hc1d-1-1/add:0\", shape=(?, ?, 512), dtype=float32)\n",
      "conv1d inputs = Tensor(\"Text2Mel/TextEnc/hc1d-1-1/add:0\", shape=(?, ?, 512), dtype=float32)\n",
      "CONV1D inputs = Tensor(\"Text2Mel/TextEnc/hc1d-2-1/add:0\", shape=(?, ?, 512), dtype=float32)\n",
      "conv1d inputs = Tensor(\"Text2Mel/TextEnc/hc1d-2-1/add:0\", shape=(?, ?, 512), dtype=float32)\n",
      "CONV1D inputs = Tensor(\"Text2Mel/TextEnc/hc1d-3-1/add:0\", shape=(?, ?, 512), dtype=float32)\n",
      "conv1d inputs = Tensor(\"Text2Mel/TextEnc/hc1d-3-1/add:0\", shape=(?, ?, 512), dtype=float32)\n",
      "CONV1D inputs = Tensor(\"Text2Mel/TextEnc/hc1d-4-1/add:0\", shape=(?, ?, 512), dtype=float32)\n",
      "conv1d inputs = Tensor(\"Text2Mel/TextEnc/hc1d-4-1/add:0\", shape=(?, ?, 512), dtype=float32)\n",
      "CONV1D inputs = Tensor(\"Text2Mel/TextEnc/hc1d-11-0/add:0\", shape=(?, ?, 512), dtype=float32)\n",
      "conv1d inputs = Tensor(\"Text2Mel/TextEnc/hc1d-11-0/add:0\", shape=(?, ?, 512), dtype=float32)\n",
      "CONV1D inputs = Tensor(\"Text2Mel/TextEnc/hc1d-11-1/add:0\", shape=(?, ?, 512), dtype=float32)\n",
      "conv1d inputs = Tensor(\"Text2Mel/TextEnc/hc1d-11-1/add:0\", shape=(?, ?, 512), dtype=float32)\n",
      "CONV1D inputs = Tensor(\"Text2Mel/TextEnc/hc1d-12-0/add:0\", shape=(?, ?, 512), dtype=float32)\n",
      "conv1d inputs = Tensor(\"Text2Mel/TextEnc/hc1d-12-0/add:0\", shape=(?, ?, 512), dtype=float32)\n",
      "\n",
      "Text Encoder Output = Tensor(\"Text2Mel/TextEnc/hc1d-12-1/add:0\", shape=(?, ?, 512), dtype=float32) \n",
      "K = Tensor(\"Text2Mel/TextEnc/split:0\", shape=(?, ?, 256), dtype=float32) \n",
      "V = Tensor(\"Text2Mel/TextEnc/split:1\", shape=(?, ?, 256), dtype=float32)\n",
      "CONV1D inputs = Tensor(\"concat:0\", shape=(?, 217, 80), dtype=float32)\n",
      "conv1d inputs = Tensor(\"Text2Mel/AudioEnc/c1d-1/c1d-1/Pad:0\", shape=(?, 217, 80), dtype=float32)\n",
      "CONV1D inputs = Tensor(\"Text2Mel/AudioEnc/c1d-1/dropout/dropout/mul_1:0\", shape=(?, 217, 256), dtype=float32)\n",
      "conv1d inputs = Tensor(\"Text2Mel/AudioEnc/c1d-2/c1d-2/Pad:0\", shape=(?, 217, 256), dtype=float32)\n",
      "CONV1D inputs = Tensor(\"Text2Mel/AudioEnc/c1d-2/dropout/dropout/mul_1:0\", shape=(?, 217, 256), dtype=float32)\n",
      "conv1d inputs = Tensor(\"Text2Mel/AudioEnc/c1d-3/c1d-3/Pad:0\", shape=(?, 217, 256), dtype=float32)\n",
      "CONV1D inputs = Tensor(\"Text2Mel/AudioEnc/c1d-3/dropout/dropout/mul_1:0\", shape=(?, 217, 256), dtype=float32)\n",
      "conv1d inputs = Tensor(\"Text2Mel/AudioEnc/hc1d-1-0/c1d-H/c1d-H/Pad:0\", shape=(?, 219, 256), dtype=float32)\n",
      "CONV1D inputs = Tensor(\"Text2Mel/AudioEnc/hc1d-1-0/add:0\", shape=(?, 217, 256), dtype=float32)\n",
      "conv1d inputs = Tensor(\"Text2Mel/AudioEnc/hc1d-2-0/c1d-H/c1d-H/Pad:0\", shape=(?, 223, 256), dtype=float32)\n",
      "CONV1D inputs = Tensor(\"Text2Mel/AudioEnc/hc1d-2-0/add:0\", shape=(?, 217, 256), dtype=float32)\n",
      "conv1d inputs = Tensor(\"Text2Mel/AudioEnc/hc1d-3-0/c1d-H/c1d-H/Pad:0\", shape=(?, 235, 256), dtype=float32)\n",
      "CONV1D inputs = Tensor(\"Text2Mel/AudioEnc/hc1d-3-0/add:0\", shape=(?, 217, 256), dtype=float32)\n",
      "conv1d inputs = Tensor(\"Text2Mel/AudioEnc/hc1d-4-0/c1d-H/c1d-H/Pad:0\", shape=(?, 271, 256), dtype=float32)\n",
      "CONV1D inputs = Tensor(\"Text2Mel/AudioEnc/hc1d-4-0/add:0\", shape=(?, 217, 256), dtype=float32)\n",
      "conv1d inputs = Tensor(\"Text2Mel/AudioEnc/hc1d-1-1/c1d-H/c1d-H/Pad:0\", shape=(?, 219, 256), dtype=float32)\n",
      "CONV1D inputs = Tensor(\"Text2Mel/AudioEnc/hc1d-1-1/add:0\", shape=(?, 217, 256), dtype=float32)\n",
      "conv1d inputs = Tensor(\"Text2Mel/AudioEnc/hc1d-2-1/c1d-H/c1d-H/Pad:0\", shape=(?, 223, 256), dtype=float32)\n",
      "CONV1D inputs = Tensor(\"Text2Mel/AudioEnc/hc1d-2-1/add:0\", shape=(?, 217, 256), dtype=float32)\n",
      "conv1d inputs = Tensor(\"Text2Mel/AudioEnc/hc1d-3-1/c1d-H/c1d-H/Pad:0\", shape=(?, 235, 256), dtype=float32)\n",
      "CONV1D inputs = Tensor(\"Text2Mel/AudioEnc/hc1d-3-1/add:0\", shape=(?, 217, 256), dtype=float32)\n",
      "conv1d inputs = Tensor(\"Text2Mel/AudioEnc/hc1d-4-1/c1d-H/c1d-H/Pad:0\", shape=(?, 271, 256), dtype=float32)\n",
      "CONV1D inputs = Tensor(\"Text2Mel/AudioEnc/hc1d-4-1/add:0\", shape=(?, 217, 256), dtype=float32)\n",
      "conv1d inputs = Tensor(\"Text2Mel/AudioEnc/hc1d-11-0/c1d-H/c1d-H/Pad:0\", shape=(?, 223, 256), dtype=float32)\n",
      "CONV1D inputs = Tensor(\"Text2Mel/AudioEnc/hc1d-11-0/add:0\", shape=(?, 217, 256), dtype=float32)\n",
      "conv1d inputs = Tensor(\"Text2Mel/AudioEnc/hc1d-11-1/c1d-H/c1d-H/Pad:0\", shape=(?, 223, 256), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0517 15:27:06.031591 12536 deprecation.py:323] From <ipython-input-9-a00be408c0e7>:151: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W0517 15:27:06.052578 12536 deprecation.py:506] From <ipython-input-9-a00be408c0e7>:152: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Audio Encoder Output = Tensor(\"Text2Mel/AudioEnc/hc1d-11-1/add:0\", shape=(?, 217, 256), dtype=float32) \n",
      "Q = Tensor(\"Text2Mel/AudioEnc/hc1d-11-1/add:0\", shape=(?, 217, 256), dtype=float32)\n",
      "\n",
      "KT = Tensor(\"Text2Mel/transpose:0\", shape=(?, 256, ?), dtype=float32) \n",
      "VT = Tensor(\"Text2Mel/transpose_1:0\", shape=(?, 256, ?), dtype=float32) \n",
      "QT = Tensor(\"Text2Mel/transpose_2:0\", shape=(?, 256, 217), dtype=float32) \n",
      "A = Tensor(\"Text2Mel/transpose_4:0\", shape=(?, ?, 217), dtype=float32)\n",
      "\n",
      "R = Tensor(\"Text2Mel/MatMul_1:0\", shape=(?, 256, 217), dtype=float32) \n",
      "RT = Tensor(\"Text2Mel/transpose_5:0\", shape=(?, 217, 256), dtype=float32) \n",
      "Rhat = Tensor(\"Text2Mel/concat_2:0\", shape=(?, 217, 512), dtype=float32)\n",
      "CONV1D inputs = Tensor(\"Text2Mel/concat_2:0\", shape=(?, 217, 512), dtype=float32)\n",
      "conv1d inputs = Tensor(\"Text2Mel/AudioDec/c1d-1/c1d-1/Pad:0\", shape=(?, 217, 512), dtype=float32)\n",
      "CONV1D inputs = Tensor(\"Text2Mel/AudioDec/c1d-1/dropout/dropout/mul_1:0\", shape=(?, 217, 256), dtype=float32)\n",
      "conv1d inputs = Tensor(\"Text2Mel/AudioDec/hc1d-1/c1d-H/c1d-H/Pad:0\", shape=(?, 219, 256), dtype=float32)\n",
      "CONV1D inputs = Tensor(\"Text2Mel/AudioDec/hc1d-1/add:0\", shape=(?, 217, 256), dtype=float32)\n",
      "conv1d inputs = Tensor(\"Text2Mel/AudioDec/hc1d-2/c1d-H/c1d-H/Pad:0\", shape=(?, 223, 256), dtype=float32)\n",
      "CONV1D inputs = Tensor(\"Text2Mel/AudioDec/hc1d-2/add:0\", shape=(?, 217, 256), dtype=float32)\n",
      "conv1d inputs = Tensor(\"Text2Mel/AudioDec/hc1d-3/c1d-H/c1d-H/Pad:0\", shape=(?, 235, 256), dtype=float32)\n",
      "CONV1D inputs = Tensor(\"Text2Mel/AudioDec/hc1d-3/add:0\", shape=(?, 217, 256), dtype=float32)\n",
      "conv1d inputs = Tensor(\"Text2Mel/AudioDec/hc1d-4/c1d-H/c1d-H/Pad:0\", shape=(?, 271, 256), dtype=float32)\n",
      "CONV1D inputs = Tensor(\"Text2Mel/AudioDec/hc1d-4/add:0\", shape=(?, 217, 256), dtype=float32)\n",
      "conv1d inputs = Tensor(\"Text2Mel/AudioDec/hc1d-5-0/c1d-H/c1d-H/Pad:0\", shape=(?, 219, 256), dtype=float32)\n",
      "CONV1D inputs = Tensor(\"Text2Mel/AudioDec/hc1d-5-0/add:0\", shape=(?, 217, 256), dtype=float32)\n",
      "conv1d inputs = Tensor(\"Text2Mel/AudioDec/hc1d-5-1/c1d-H/c1d-H/Pad:0\", shape=(?, 219, 256), dtype=float32)\n",
      "CONV1D inputs = Tensor(\"Text2Mel/AudioDec/hc1d-5-1/add:0\", shape=(?, 217, 256), dtype=float32)\n",
      "conv1d inputs = Tensor(\"Text2Mel/AudioDec/c1d-2-0/c1d-2-0/Pad:0\", shape=(?, 217, 256), dtype=float32)\n",
      "CONV1D inputs = Tensor(\"Text2Mel/AudioDec/c1d-2-0/Relu:0\", shape=(?, 217, 256), dtype=float32)\n",
      "conv1d inputs = Tensor(\"Text2Mel/AudioDec/c1d-2-1/c1d-2-1/Pad:0\", shape=(?, 217, 256), dtype=float32)\n",
      "CONV1D inputs = Tensor(\"Text2Mel/AudioDec/c1d-2-1/Relu:0\", shape=(?, 217, 256), dtype=float32)\n",
      "conv1d inputs = Tensor(\"Text2Mel/AudioDec/c1d-2-2/c1d-2-2/Pad:0\", shape=(?, 217, 256), dtype=float32)\n",
      "CONV1D inputs = Tensor(\"Text2Mel/AudioDec/c1d-2-2/Relu:0\", shape=(?, 217, 256), dtype=float32)\n",
      "conv1d inputs = Tensor(\"Text2Mel/AudioDec/c1d-3/c1d-3/Pad:0\", shape=(?, 217, 256), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0517 15:27:10.687839 12536 deprecation.py:323] From c:\\users\\abhishek\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0517 15:27:27.591143 12536 deprecation.py:323] From <ipython-input-9-a00be408c0e7>:254: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.MonitoredTrainingSession\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Graph loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0517 15:29:29.121789  9324 meta_graph.py:449] Issue encountered when serializing global_step.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'Tensor' object has no attribute 'to_proto'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Step 1       : loss=1.01799,l1=0.29260,bin=0.69315,A_loss=0.00322"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-a00be408c0e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    257\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m             gs,l_m,l_m_l1,l_m_b,l_A,ops = sess.run([g.global_step,\n\u001b[1;32m--> 259\u001b[1;33m                 g.loss_mels,g.mel_l1_loss,g.mel_bin_div,g.A_loss,g.train_mel])\n\u001b[0m\u001b[0;32m    260\u001b[0m             \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Step %-7d : loss=%.05f,l1=%.05f,bin=%.05f,A_loss=%.05f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ml_m\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ml_m_l1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ml_m_b\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ml_A\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m             \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\r'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\abhishek\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 950\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    951\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\abhishek\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1171\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1173\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1174\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\abhishek\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1350\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\abhishek\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1354\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1355\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1356\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1357\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\abhishek\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1341\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\abhishek\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1429\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1431\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import re\n",
    "#import audio\n",
    "\n",
    "def load_vocab():\n",
    "\n",
    "    # characters = \"PEاإأآبتثجحخدذرزسشصضطظعغفقكلمنهويىؤءةئ ًٌٍَُِّْ،.\" # Arabic character set\n",
    "    characters = \"PE abcdefghijklmnopqrstuvwxyz'.,?\"  # P: Padding E: End of Sentence\n",
    "\n",
    "    char2idx = {char: idx for idx, char in enumerate(characters)}\n",
    "    idx2char = {idx: char for idx, char in enumerate(characters)}\n",
    "    return char2idx, idx2char\n",
    "\n",
    "def clean(text):\n",
    "    text=text.lower()\n",
    "    re_list = r\"[^ abcdefghijklmnopqrstuvwxyz'.,?]\" # E: Empty. ignore G\n",
    "    #re_list = r\"[^اإأآبتثجحخدذرزسشصضطظعغفقكلمنهويىؤءةئ ًٌٍَُِّْ،.]\" # Arabic character set\n",
    "    _text = re.sub(re_list, \"\", text)\n",
    "    return(_text)\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    def mypyfunc(text):\n",
    "        text = text.decode(\"utf-8\")\n",
    "        items = text.split(\"|\")\n",
    "        char2idx,_=load_vocab()\n",
    "        text = items[1]\n",
    "        text = clean(text)\n",
    "        source = [char2idx[c] for c in text+'E']\n",
    "        dest = items[0]\n",
    "        mels = np.load(os.path.join(data_dir, \"mels\", dest + \".npy\"))\n",
    "        mels = mels[::4,:]\n",
    "        return np.array(source, dtype=np.int32),mels\n",
    "    def _pad(text,mel):\n",
    "        text = tf.pad(text, ((0, maxlen),))[:maxlen] # (Tx,)\n",
    "        mel = tf.pad(mel, ((0, Tyr), (0, 0)))[:Tyr] # (Tyr, n_mels)\n",
    "        return text,mel\n",
    "    dataset = tf.data.TextLineDataset(tf.convert_to_tensor(metafile))\n",
    "    dataset = dataset.map(lambda text: tuple(tf.py_func(mypyfunc, [text], [tf.int32, tf.float32])))\n",
    "    dataset = dataset.map(_pad)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.shuffle(buffer_size=400)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    next_element = iterator.get_next()\n",
    "    print(next_element)\n",
    "    return(next_element)\n",
    "\n",
    "\n",
    "\n",
    "def w_fun(n, t):\n",
    "    return 1 - np.exp(-((n/(maxlen-1) - t/(Tyr-1))**2) / (2 * g**2))\n",
    "\n",
    "def guide_fn(x):\n",
    "    prva=-1\n",
    "    #return(x)\n",
    "    f=40\n",
    "    if x.shape[1]<=f:\n",
    "        return(x)\n",
    "    prva = np.argmax(x[:,f])-1\n",
    "    for i in range(f,x.shape[1]):\n",
    "\n",
    "        pos = np.argmax(x[:,i])\n",
    "        val = x[pos,i]\n",
    "        if (pos<prva) or (pos>prva+1):\n",
    "            x[:,i]=np.zeros(x.shape[0],dtype='f')\n",
    "            pp = min(x.shape[0]-1,prva+1)\n",
    "            x[pp,i]=1\n",
    "            #print(\"%d-Corrected from %d to %d - prva %d\"%(i,pos,pp,prva))\n",
    "        else:\n",
    "            x[:,i]=np.zeros(x.shape[0],dtype='f')\n",
    "            x[pos,i]=1\n",
    "            pass\n",
    "            #print(\"%d-Was ok %d - prva %d\"%(i,pos,prva))\n",
    "        prva=np.argmax(x[:,i])\n",
    "    return x\n",
    "\n",
    "\n",
    "def guide_atten(inputs): # 180,XX\n",
    "    return tf.py_func(guide_fn,[inputs],tf.float32)\n",
    "\n",
    "class Graph_Text2Mel():\n",
    "    def __init__(self, is_training=True):\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            if is_training:\n",
    "                self.text, self.mel = get_data() # (N, T), (N,Tyr,nmels)\n",
    "                self.mel = tf.reshape(self.mel,shape=[-1,Tyr,n_mels])\n",
    "                w = np.fromfunction(w_fun, (maxlen, Tyr), dtype='f')\n",
    "                w = np.expand_dims(w,0)\n",
    "                w = np.repeat(w,batch_size,0)\n",
    "                self.A_guide = tf.convert_to_tensor(w) # B,180,870\n",
    "            else: # inference\n",
    "                self.text = tf.placeholder(tf.int32, shape=(None, maxlen))\n",
    "                self.mel = tf.placeholder(tf.float32, shape=(None,None,n_mels))\n",
    "\n",
    "            # define decoder inputs\n",
    "            if is_training:\n",
    "                self.decoder_inputs = tf.concat((tf.zeros_like(self.mel[:, :1,:]), self.mel[:, :-1,:]), 1) # shift mels to right\n",
    "            else:\n",
    "                self.decoder_inputs=self.mel\n",
    "                print(self.mel)\n",
    "            char2idx, idx2char = load_vocab()\n",
    "            with tf.variable_scope(\"Text2Mel\"):\n",
    "                with tf.variable_scope(\"TextEnc\"):\n",
    "                    self.emb=embedding(self.text,\n",
    "                                        vocab_size=len(char2idx), \n",
    "                                        num_units=e,\n",
    "                                        scale = False,\n",
    "                                        scope=\"embedding\") #in (N,T) out (N,T,e) (32,180,128)\n",
    "                    self.textenc=Conv1D(self.emb,d*2,1,1,causal=False,is_training=is_training,activation=tf.nn.relu,scope='c1d-1')\n",
    "                    self.textenc=Conv1D(self.textenc,d*2,1,1,causal=False,is_training=is_training,scope='c1d-2')\n",
    "                    for i in range(2):\n",
    "                        self.textenc=HConv1D(self.textenc,d*2,3,1,causal=False,is_training=is_training,scope='hc1d-1-%d'%i)\n",
    "                        self.textenc=HConv1D(self.textenc,d*2,3,3,causal=False,is_training=is_training,scope='hc1d-2-%d'%i)\n",
    "                        self.textenc=HConv1D(self.textenc,d*2,3,9,causal=False,is_training=is_training,scope='hc1d-3-%d'%i)\n",
    "                        self.textenc=HConv1D(self.textenc,d*2,3,27,causal=False,is_training=is_training,scope='hc1d-4-%d'%i)\n",
    "                    for i in range(2):\n",
    "                        self.textenc=HConv1D(self.textenc,d*2,3,1,causal=False,is_training=is_training,scope='hc1d-11-%d'%i)\n",
    "                    for i in range(2):\n",
    "                        self.textenc=HConv1D(self.textenc,d*2,1,1,causal=False,is_training=is_training,scope='hc1d-12-%d'%i) #(N,T,2*d) (32,180,512)\n",
    "\n",
    "\n",
    "                    self.K,self.V = tf.split(self.textenc,num_or_size_splits=2,axis=2)  #k=(B,N,d) v=(B,N,d)\n",
    "                    print(\"\\nText Encoder Output = {} \\nK = {} \\nV = {}\".format(self.textenc,self.K,self.V))\n",
    "                with tf.variable_scope(\"AudioEnc\"):\n",
    "                    self.audioenc = Conv1D(self.decoder_inputs,d,1,1,is_training=is_training,activation=tf.nn.relu,scope='c1d-1') # from (B,Ty,80) -> (B,Ty,d)\n",
    "                    self.audioenc = Conv1D(self.audioenc,d,1,1,is_training=is_training,activation=tf.nn.relu,scope='c1d-2')\n",
    "                    self.audioenc = Conv1D(self.audioenc,d,1,1,is_training=is_training,scope='c1d-3')\n",
    "                    for i in range(2):\n",
    "                        self.audioenc=HConv1D(self.audioenc,d,3,1,is_training=is_training,scope='hc1d-1-%d'%i)\n",
    "                        self.audioenc=HConv1D(self.audioenc,d,3,3,is_training=is_training,scope='hc1d-2-%d'%i)\n",
    "                        self.audioenc=HConv1D(self.audioenc,d,3,9,is_training=is_training,scope='hc1d-3-%d'%i)\n",
    "                        self.audioenc=HConv1D(self.audioenc,d,3,27,is_training=is_training,scope='hc1d-4-%d'%i)\n",
    "                    for i in range(2):\n",
    "                        self.audioenc=HConv1D(self.audioenc,d,3,3,is_training=is_training,scope='hc1d-11-%d'%i)\n",
    "                    self.Q = self.audioenc                  # (B,Ty,d)\n",
    "                    print(\"\\n Audio Encoder Output = {} \\nQ = {}\".format(self.audioenc,self.Q))\n",
    "\n",
    "                self.KT = tf.transpose(self.K,perm=[0,2,1]) # B,d,180\n",
    "                self.VT = tf.transpose(self.V,perm=[0,2,1]) # B,d,180\n",
    "                self.QT = tf.transpose(self.Q,perm=[0,2,1]) # B,d,870\n",
    "\n",
    "                self.A = tf.matmul(self.K,self.QT)    # (B,180,d) * (B,d,870) = (B,180,870)\n",
    "                self.A *= tf.sqrt(1/tf.to_float(d))\n",
    "                self.A = tf.nn.softmax(self.A,dim=1) #B,180,870\n",
    "                \n",
    "                print(\"\\nKT = {} \\nVT = {} \\nQT = {} \\nA = {}\".format(self.KT,self.VT,self.QT,self.A))\n",
    "                if not is_training:\n",
    "                    self.A = tf.map_fn(guide_atten,self.A,parallel_iterations=1)\n",
    "                    print(\"A = \",self.A)\n",
    "                    \n",
    "                self.R = tf.matmul(self.VT,self.A)      # B,d,180 * B,180,870 -> B,d,870\n",
    "                self.RT = tf.transpose(self.R,perm=[0,2,1]) # B,870,d\n",
    "                self.Rhat = tf.concat((self.RT,self.Q),2)   # (B,Ty,d),(B,Ty,d) --> (B,Ty,2d)\n",
    "                print(\"\\nR = {} \\nRT = {} \\nRhat = {}\".format(self.R,self.RT,self.Rhat))\n",
    "                \n",
    "                with tf.variable_scope(\"AudioDec\"):\n",
    "                    self.audiodec = Conv1D(self.Rhat,d,1,1,is_training=is_training,scope='c1d-1')\n",
    "                    self.audiodec=HConv1D(self.audiodec,d,3,1,is_training=is_training,scope='hc1d-1')\n",
    "                    self.audiodec=HConv1D(self.audiodec,d,3,3,is_training=is_training,scope='hc1d-2')\n",
    "                    self.audiodec=HConv1D(self.audiodec,d,3,9,is_training=is_training,scope='hc1d-3')\n",
    "                    self.audiodec=HConv1D(self.audiodec,d,3,27,is_training=is_training,scope='hc1d-4')\n",
    "                    for i in range(2):\n",
    "                        self.audiodec=HConv1D(self.audiodec,d,3,1,is_training=is_training,scope='hc1d-5-%d'%i)\n",
    "                    for i in range(3):\n",
    "                        self.audiodec=Conv1D(self.audiodec,d,1,1,dropout=0,is_training=is_training,scope='c1d-2-%d'%i,activation=tf.nn.relu)\n",
    "                    self.mel_logits = Conv1D(self.audiodec,n_mels,1,1,dropout=0,is_training=is_training,scope='c1d-3') # (B,Tyr,nmels)\n",
    "                    self.mel_output = tf.nn.sigmoid(self.mel_logits)                            #(B,Tyr,nmels)\n",
    "\n",
    "            if is_training:  \n",
    "                # Loss\n",
    "                self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "                #self.learning_rate = _learning_rate_decay(self.global_step)\n",
    "\n",
    "                #self.learning_rate = tf.train.exponential_decay(lr,self.global_step,1500,0.9)\n",
    "                self.learning_rate = lr//4\n",
    "                if masking:\n",
    "                    self.is_target = tf.to_float(tf.not_equal(self.mel,0))\n",
    "                    #self.mel_l1_loss = tf.reduce_mean(tf.abs(self.mel-self.mel_output))\n",
    "                    self.mel_l1_loss = tf.reduce_sum(tf.abs(self.mel-self.mel_output)*self.is_target)/tf.reduce_sum(self.is_target)\n",
    "\n",
    "                    #self.mel_bin_div = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.mel_logits,labels=self.mel))\n",
    "                    self.mel_bin_div = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.mel_logits,labels=self.mel)\n",
    "                    self.mel_bin_div = tf.reduce_sum(self.mel_bin_div*self.is_target)/tf.reduce_sum(self.is_target)\n",
    "                else:\n",
    "                    self.mel_l1_loss = tf.reduce_mean(tf.abs(self.mel-self.mel_output))\n",
    "                    self.mel_bin_div = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.mel_logits,labels=self.mel))\n",
    "\n",
    "                self.A_loss = tf.reduce_mean(self.A_guide*self.A)\n",
    "\n",
    "\n",
    "                self.loss_mels = self.mel_l1_loss + self.mel_bin_div + 10*self.A_loss\n",
    "                self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate, beta1=b1, beta2=b2, epsilon=eps)\n",
    "                #self.gvs = self.optimizer.compute_gradients(self.loss_mels) \n",
    "                #self.clipped = []\n",
    "                #for grad, var in self.gvs:\n",
    "                    #if grad is not None:\n",
    "                        #grad = tf.clip_by_norm(grad, max_grad_norm)\n",
    "    \n",
    "                    #self.clipped.append((grad, var))\n",
    "                self.train_mel = self.optimizer.minimize(self.loss_mels,global_step=self.global_step)\n",
    "                tf.summary.scalar('loss_mels', self.loss_mels)\n",
    "                tf.summary.scalar('loss_mel_l1', self.mel_l1_loss)\n",
    "                tf.summary.scalar('loss_mel_binary', self.mel_bin_div)\n",
    "                tf.summary.scalar('loss_Attention', self.A_loss)\n",
    "                tf.summary.scalar('learning_rate', self.learning_rate)\n",
    "            self.merged = tf.summary.merge_all()\n",
    "\n",
    "def show(mel1,mel2,name):\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.imshow(np.transpose(mel1),interpolation='nearest',  cmap=plt.cm.afmhot, origin='lower')\n",
    "    plt.title(\"Generated\")\n",
    "    plt.colorbar()\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.imshow(np.transpose(mel2),interpolation='nearest',  cmap=plt.cm.afmhot, origin='lower')\n",
    "    plt.title(\"Original\")\n",
    "    plt.colorbar()\n",
    "    plt.savefig(name)\n",
    "    plt.cla()\n",
    "    plt.close('all')\n",
    "\n",
    "\n",
    "def showmels(mel,msg,file):\n",
    "    fig, ax = plt.subplots(nrows=1,ncols=1, figsize=(8,4))\n",
    "    cax = ax.matshow(mel, interpolation='nearest',  cmap=plt.cm.afmhot, origin='lower')\n",
    "    fig.colorbar(cax)\n",
    "    plt.title(msg+str(len(msg)))\n",
    "    plt.savefig(file,format='png')\n",
    "    plt.cla()\n",
    "    plt.close('all')\n",
    "\n",
    "\n",
    "def _learning_rate_decay(global_step):\n",
    "    # Noam scheme from tensor2tensor:\n",
    "    step = tf.cast(global_step + 1, dtype=tf.float32)\n",
    "    return init_lr * warmup_steps**0.5 * tf.minimum(step * warmup_steps**-1.5, step**-0.5)\n",
    "\n",
    "def tdecode(text):\n",
    "    char2idx,idx2char=load_vocab()\n",
    "    return(\"\".join(idx2char[i] for i in text).split('P')[0])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':  \n",
    "    g = Graph_Text2Mel(); print(\"Training Graph loaded\")\n",
    "    sv = tf.train.Supervisor(graph=g.graph, \n",
    "                             logdir=logdirmel,)\n",
    "                             #save_model_secs=0)\n",
    "    with sv.managed_session() as sess:\n",
    "        while not sv.should_stop():\n",
    "            gs,l_m,l_m_l1,l_m_b,l_A,ops = sess.run([g.global_step,\n",
    "                g.loss_mels,g.mel_l1_loss,g.mel_bin_div,g.A_loss,g.train_mel])\n",
    "            message = \"Step %-7d : loss=%.05f,l1=%.05f,bin=%.05f,A_loss=%.05f\" % (gs,l_m,l_m_l1,l_m_b,l_A)\n",
    "            sys.stdout.write('\\r'+message)\n",
    "            sys.stdout.flush()\n",
    "            if (gs+1) % logevery == 0:\n",
    "                gs,l_m,l_m_l1,l_m_b,l_A,t_i,m_i,a,m_o,ops = sess.run([g.global_step,\n",
    "                    g.loss_mels,g.mel_l1_loss,g.mel_bin_div,g.A_loss,g.text,g.mel,g.A,g.mel_output,g.train_mel])\n",
    "                message = \"Step %-7d : loss=%.05f,l1=%.05f,bin=%.05f,A_loss=%.05f\" % (gs,l_m,l_m_l1,l_m_b,l_A)\n",
    "                sys.stdout.write('\\r'+message)\n",
    "                sys.stdout.flush()\n",
    "                show(m_o[0],m_i[0],\"mel0.png\")\n",
    "                show(m_o[1],m_i[1],\"mel1.png\")\n",
    "                showmels(a[0],tdecode(t_i[0]),\"a0.png\")\n",
    "                showmels(a[1],tdecode(t_i[1]),\"a1.png\")\n",
    "\n",
    "    print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YCQHYrRF1QS_"
   },
   "source": [
    "<h1>SPECTOGRAM SUPER RESOLUTION MODEL</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 628
    },
    "colab_type": "code",
    "id": "uAlvBoB11QTC",
    "outputId": "f01b2fe2-d00b-41f6-d1bf-90c9ef3701bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONV1D inputs = Tensor(\"Reshape:0\", shape=(?, 217, 80), dtype=float32)\n",
      "conv1d inputs = Tensor(\"Reshape:0\", shape=(?, 217, 80), dtype=float32)\n",
      "CONV1D inputs = Tensor(\"SSRN/c1d-1/dropout/dropout/mul_1:0\", shape=(?, 217, 512), dtype=float32)\n",
      "conv1d inputs = Tensor(\"SSRN/c1d-1/dropout/dropout/mul_1:0\", shape=(?, 217, 512), dtype=float32)\n",
      "CONV1D inputs = Tensor(\"SSRN/hc1d-1/add:0\", shape=(?, 217, 512), dtype=float32)\n",
      "conv1d inputs = Tensor(\"SSRN/hc1d-1/add:0\", shape=(?, 217, 512), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0517 15:32:33.747102 12536 deprecation.py:323] From <ipython-input-8-3139ff40e5d6>:121: conv2d_transpose (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2DTranspose` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONV1D inputs = Tensor(\"SSRN/deconv-0/Squeeze:0\", shape=(?, 434, 512), dtype=float32)\n",
      "conv1d inputs = Tensor(\"SSRN/deconv-0/Squeeze:0\", shape=(?, 434, 512), dtype=float32)\n",
      "CONV1D inputs = Tensor(\"SSRN/hc1d-31-0/add:0\", shape=(?, 434, 512), dtype=float32)\n",
      "conv1d inputs = Tensor(\"SSRN/hc1d-31-0/add:0\", shape=(?, 434, 512), dtype=float32)\n",
      "CONV1D inputs = Tensor(\"SSRN/deconv-1/Squeeze:0\", shape=(?, 868, 512), dtype=float32)\n",
      "conv1d inputs = Tensor(\"SSRN/deconv-1/Squeeze:0\", shape=(?, 868, 512), dtype=float32)\n",
      "CONV1D inputs = Tensor(\"SSRN/hc1d-31-1/add:0\", shape=(?, 868, 512), dtype=float32)\n",
      "conv1d inputs = Tensor(\"SSRN/hc1d-31-1/add:0\", shape=(?, 868, 512), dtype=float32)\n",
      "CONV1D inputs = Tensor(\"SSRN/hc1d-32-1/add:0\", shape=(?, 868, 512), dtype=float32)\n",
      "conv1d inputs = Tensor(\"SSRN/hc1d-32-1/add:0\", shape=(?, 868, 512), dtype=float32)\n",
      "CONV1D inputs = Tensor(\"SSRN/c1d-2/dropout/dropout/mul_1:0\", shape=(?, 868, 1024), dtype=float32)\n",
      "conv1d inputs = Tensor(\"SSRN/c1d-2/dropout/dropout/mul_1:0\", shape=(?, 868, 1024), dtype=float32)\n",
      "CONV1D inputs = Tensor(\"SSRN/hc1d-4-0/add:0\", shape=(?, 868, 1024), dtype=float32)\n",
      "conv1d inputs = Tensor(\"SSRN/hc1d-4-0/add:0\", shape=(?, 868, 1024), dtype=float32)\n",
      "CONV1D inputs = Tensor(\"SSRN/hc1d-4-1/add:0\", shape=(?, 868, 1024), dtype=float32)\n",
      "conv1d inputs = Tensor(\"SSRN/hc1d-4-1/add:0\", shape=(?, 868, 1024), dtype=float32)\n",
      "CONV1D inputs = Tensor(\"SSRN/c1d-3/dropout/dropout/mul_1:0\", shape=(?, 868, 1025), dtype=float32)\n",
      "conv1d inputs = Tensor(\"SSRN/c1d-3/dropout/dropout/mul_1:0\", shape=(?, 868, 1025), dtype=float32)\n",
      "CONV1D inputs = Tensor(\"SSRN/c1d-4-0/dropout/dropout/mul_1:0\", shape=(?, 868, 1025), dtype=float32)\n",
      "conv1d inputs = Tensor(\"SSRN/c1d-4-0/dropout/dropout/mul_1:0\", shape=(?, 868, 1025), dtype=float32)\n",
      "CONV1D inputs = Tensor(\"SSRN/c1d-4-1/dropout/dropout/mul_1:0\", shape=(?, 868, 1025), dtype=float32)\n",
      "conv1d inputs = Tensor(\"SSRN/c1d-4-1/dropout/dropout/mul_1:0\", shape=(?, 868, 1025), dtype=float32)\n",
      "Training Graph loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0517 15:33:39.176406 19284 meta_graph.py:449] Issue encountered when serializing global_step.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'Tensor' object has no attribute 'to_proto'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Step 1 : l=0.94497 (Ml1=0.25182,Mb=0.69315)"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\abhishek\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\training\\supervisor.py\u001b[0m in \u001b[0;36mmanaged_session\u001b[1;34m(self, master, config, start_standard_services, close_summary_writer)\u001b[0m\n\u001b[0;32m   1003\u001b[0m           start_standard_services=start_standard_services)\n\u001b[1;32m-> 1004\u001b[1;33m       \u001b[1;32myield\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1005\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-30da31b5104a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    137\u001b[0m             gs,l_M,l_M_l1,l_M_b,ops = sess.run([g.global_step,\n\u001b[1;32m--> 138\u001b[1;33m                 g.loss_mags,g.mag_l1_loss,g.mag_bin_div,g.train_mag])\n\u001b[0m\u001b[0;32m    139\u001b[0m             \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Step %d : l=%.05f (Ml1=%.05f,Mb=%.05f)\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ml_M\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ml_M_l1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ml_M_b\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\abhishek\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 950\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    951\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\abhishek\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1173\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1174\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\abhishek\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1350\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\abhishek\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1355\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1356\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1357\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\abhishek\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1341\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\abhishek\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1429\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-30da31b5104a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    151\u001b[0m                 \u001b[1;31m#audio.save_spec(M_o[1].T,\"out1.wav\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m                 \u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM_o\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mM_i\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"mag0.png\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m                 \u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM_o\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mM_i\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"mag1.png\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\abhishek\\appdata\\local\\programs\\python\\python37\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\abhishek\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\training\\supervisor.py\u001b[0m in \u001b[0;36mmanaged_session\u001b[1;34m(self, master, config, start_standard_services, close_summary_writer)\u001b[0m\n\u001b[0;32m   1012\u001b[0m         \u001b[1;31m# threads which are not checking for `should_stop()`.  They\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[1;31m# will be stopped when we close the session further down.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclose_summary_writer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclose_summary_writer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1015\u001b[0m       \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m         \u001b[1;31m# Close the session to finish up all pending calls.  We do not care\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\abhishek\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\training\\supervisor.py\u001b[0m in \u001b[0;36mstop\u001b[1;34m(self, threads, close_summary_writer, ignore_live_threads)\u001b[0m\n\u001b[0;32m    837\u001b[0m           \u001b[0mthreads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    838\u001b[0m           \u001b[0mstop_grace_period_secs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stop_grace_secs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 839\u001b[1;33m           ignore_live_threads=ignore_live_threads)\n\u001b[0m\u001b[0;32m    840\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    841\u001b[0m       \u001b[1;31m# Close the writer last, in case one of the running threads was using it.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\abhishek\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\training\\coordinator.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(self, threads, stop_grace_period_secs, ignore_live_threads)\u001b[0m\n\u001b[0;32m    371\u001b[0m     \u001b[0mstop_wait_secs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthreads\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mstop_grace_period_secs\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 373\u001b[1;33m       \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstop_wait_secs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    374\u001b[0m       \u001b[0mstop_grace_period_secs\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mstop_wait_secs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m       \u001b[0mstop_wait_secs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mstop_wait_secs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "#import audio\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    def mypyfunc(text):\n",
    "        text = text.decode(\"utf-8\")\n",
    "        items = text.split(\"|\")\n",
    "        dest = items[0]\n",
    "        mels = np.load(os.path.join(data_dir, \"mels\", dest + \".npy\"))\n",
    "        mels = mels[::4,:]\n",
    "        mags = np.load(os.path.join(data_dir, \"mags\", dest + \".npy\"))\n",
    "        return mels,mags\n",
    "    def _pad(mel,mag):\n",
    "        mel = tf.pad(mel, ((0, Tyr), (0, 0)))[:Tyr] # (Tyr, n_mels)\n",
    "        mag = tf.pad(mag, ((0, Ty), (0, 0)))[:Ty] # (Ty, 1+n_fft/2)\n",
    "        return mel,mag\n",
    "    dataset = tf.data.TextLineDataset(tf.convert_to_tensor(metafile))\n",
    "    dataset = dataset.map(lambda text: tuple(tf.py_func(mypyfunc, [text], [tf.float32, tf.float32])))\n",
    "    dataset = dataset.map(_pad)\n",
    "    dataset = dataset.shuffle(buffer_size=400)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    next_element = iterator.get_next()\n",
    "    return(next_element)\n",
    "\n",
    "\n",
    "class Graph_SSR():\n",
    "    def __init__(self, is_training=True):\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            if is_training:\n",
    "                self.mel, self.mag=get_data() # (N,Tyr,nmels), (N,Ty,1+n_ffts//2)\n",
    "                self.mel = tf.reshape(self.mel,shape=[-1,Tyr,n_mels])\n",
    "            else: # inference\n",
    "                self.mel = tf.placeholder(tf.float32, shape=(None,None,n_mels))\n",
    "            with tf.variable_scope(\"SSRN\"):\n",
    "                self.ssrn = Conv1D(self.mel,c,1,1,causal=False,is_training=is_training,scope='c1d-1')\n",
    "                self.ssrn = HConv1D(self.ssrn,c,3,1,causal=False,is_training=is_training,scope='hc1d-1')\n",
    "                self.ssrn = HConv1D(self.ssrn,c,3,3,causal=False,is_training=is_training,scope='hc1d-2')\n",
    "                for i in range(2):\n",
    "                    self.ssrn = Deconv1D(self.ssrn,c,2,1,scope='deconv-%d'%i)\n",
    "                    self.ssrn = HConv1D(self.ssrn,c,3,1,causal=False,is_training=is_training,scope='hc1d-31-%d'%i)\n",
    "                    self.ssrn = HConv1D(self.ssrn,c,3,3,causal=False,is_training=is_training,scope='hc1d-32-%d'%i)\n",
    "                self.ssrn = Conv1D(self.ssrn,c*2,1,1,causal=False,is_training=is_training,scope='c1d-2')\n",
    "                for i in range(2):\n",
    "                    self.ssrn=HConv1D(self.ssrn,c*2,3,1,causal=False,is_training=is_training,scope='hc1d-4-%d'%i)\n",
    "                self.ssrn = Conv1D(self.ssrn,fd,1,1,causal=False,is_training=is_training,scope='c1d-3')\n",
    "                for i in range(2):\n",
    "                    self.ssrn=Conv1D(self.ssrn,fd,1,1,causal=False,is_training=is_training,activation=tf.nn.relu,scope='c1d-4-%d'%i)\n",
    "                self.mag_logits = Conv1D(self.ssrn,fd,1,1,causal=False,is_training=is_training,scope='c1d-5')\n",
    "                self.mag_output = tf.nn.sigmoid(self.mag_logits)\n",
    "            if is_training:  \n",
    "                # Loss\n",
    "                self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "                #self.learning_rate = _learning_rate_decay(self.global_step)\n",
    "\n",
    "                #self.learning_rate = tf.train.exponential_decay(lr,self.global_step,3000,0.9)\n",
    "                self.learning_rate = lr\n",
    "\n",
    "                self.l1 = tf.abs(self.mag - self.mag_output)\n",
    "                self.n_priority = int(3000/(sr*0.5) * fd)\n",
    "                self.mag_l1_loss = 0.5*tf.reduce_mean(self.l1) + 0.5 * tf.reduce_mean(self.l1[:,:,0:self.n_priority]) \n",
    "                #self.mag_l1_loss = tf.reduce_mean(tf.abs(self.mag-self.mag_output))\n",
    "                #self.mag_l1_loss = tf.reduce_sum(tf.abs(self.mag-self.mag_output)*tf.to_float(tf.not_equal(self.mag,0)))/tf.reduce_sum(tf.to_float(tf.not_equal(self.mag,0)))\n",
    "                self.mag_bin_div = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.mag_logits,labels=self.mag))\n",
    "                #self.mag_bin_div = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.mag_logits,labels=self.mag)\n",
    "                #self.mag_bin_div = tf.reduce_sum(self.mag_bin_div*tf.to_float(tf.not_equal(self.mag,0)))/tf.reduce_sum(tf.to_float(tf.not_equal(self.mag,0)))\n",
    "\n",
    "                self.loss_mags = self.mag_l1_loss + self.mag_bin_div\n",
    "                self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate, beta1=b1, beta2=b2, epsilon=eps)\n",
    "                #self.gvs = self.optimizer.compute_gradients(self.loss_mels) \n",
    "                #self.clipped = []\n",
    "                #for grad, var in self.gvs:\n",
    "                    #if grad is not None:\n",
    "                        #grad = tf.clip_by_norm(grad, max_grad_norm)\n",
    "                        \n",
    "                    #self.clipped.append((grad, var))\n",
    "                #self.train_op = self.optimizer.apply_gradients(self.clipped, global_step=self.global_step)\n",
    "                self.train_mag = self.optimizer.minimize(self.loss_mags,global_step=self.global_step)\n",
    "                tf.summary.scalar('loss_mags', self.loss_mags)\n",
    "                tf.summary.scalar('loss_mag_binary', self.mag_bin_div)\n",
    "                tf.summary.scalar('loss_mag_l1', self.mag_l1_loss)\n",
    "                tf.summary.scalar('learning_rate', self.learning_rate)\n",
    "            else:\n",
    "                self.wav_output = inv_spectrogram_tensorflow(self.mag_output)\n",
    "            self.merged = tf.summary.merge_all()\n",
    "\n",
    "def show(mel1,mel2,name):\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.imshow(np.transpose(mel1),interpolation='nearest', aspect='auto', cmap=plt.cm.afmhot, origin='lower')\n",
    "    plt.title(\"Generated\")\n",
    "    plt.colorbar()\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.imshow(np.transpose(mel2),interpolation='nearest', aspect='auto', cmap=plt.cm.afmhot, origin='lower')\n",
    "    plt.title(\"Original\")\n",
    "    plt.colorbar()\n",
    "    plt.savefig(name)\n",
    "    plt.cla()\n",
    "    plt.close('all')\n",
    "\n",
    "      \n",
    "def showmels(mel,msg,file):\n",
    "    fig, ax = plt.subplots(nrows=1,ncols=1, figsize=(8,4))\n",
    "    cax = ax.matshow(mel, interpolation='nearest',  cmap=plt.cm.afmhot, origin='lower')\n",
    "    fig.colorbar(cax)\n",
    "    plt.title(msg+str(len(msg)))\n",
    "    plt.savefig(file,format='png')\n",
    "    plt.cla()\n",
    "    plt.close('all')\n",
    "\n",
    "def _learning_rate_decay(global_step):\n",
    "    # Noam scheme from tensor2tensor:\n",
    "    step = tf.cast(global_step + 1, dtype=tf.float32)\n",
    "    return init_lr * warmup_steps**0.5 * tf.minimum(step * warmup_steps**-1.5, step**-0.5)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    g = Graph_SSR(); print(\"Training Graph loaded\")\n",
    "    sv = tf.train.Supervisor(graph=g.graph, \n",
    "                             logdir=logdirmag,)\n",
    "                             #save_model_secs=0)\n",
    "    with sv.managed_session() as sess:\n",
    "        while not sv.should_stop():\n",
    "            gs,l_M,l_M_l1,l_M_b,ops = sess.run([g.global_step,\n",
    "                g.loss_mags,g.mag_l1_loss,g.mag_bin_div,g.train_mag])\n",
    "            message = \"Step %d : l=%.05f (Ml1=%.05f,Mb=%.05f)\" % (gs,l_M,l_M_l1,l_M_b)\n",
    "            sys.stdout.write('\\r'+message)\n",
    "            sys.stdout.flush()\n",
    "            #print(message)\n",
    "            if (gs+1) % logevery == 0:\n",
    "                gs,l_M,l_M_l1,l_M_b,M_o,M_i,ops = sess.run([g.global_step,\n",
    "                    g.loss_mags,g.mag_l1_loss,g.mag_bin_div,\n",
    "                    g.mag_output, g.mag,g.train_mag])\n",
    "                message = \"Step %d : l=%.05f (Ml1=%.05f,Mb=%.05f)\" % (gs,l_M,l_M_l1,l_M_b)\n",
    "                sys.stdout.write('\\r'+message)\n",
    "                sys.stdout.flush()\n",
    "                #audio.save_spec(M_o[0].T,\"out0.wav\")\n",
    "                #audio.save_spec(M_o[1].T,\"out1.wav\")\n",
    "                show(M_o[0],M_i[0],\"mag0.png\")\n",
    "                show(M_o[1],M_i[1],\"mag1.png\")\n",
    "            \n",
    "\n",
    "\n",
    "    print(\"Done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CfAP5bl71QTJ"
   },
   "source": [
    "<h1>Synthesization</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "colab_type": "code",
    "id": "dX7nWWfH1QTL",
    "outputId": "13c9fbc3-2029-46c7-cbea-fa50864d7606"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready To Perform Inferrence ....\n",
      "\n",
      "Enter TextHi, my name is John. what are you doing?\n",
      "Synthesization In Progress ...\n",
      "\n",
      "Synthesization Completed!\n"
     ]
    }
   ],
   "source": [
    "from gtts import gTTS \n",
    "import os\n",
    "\n",
    "class Synth:\n",
    "    def __init__(self):\n",
    "        \n",
    "        print(\"Ready To Perform Inferrence ....\\n\")\n",
    "        self.text = input(\"Enter Text\")\n",
    "        print(\"Synthesization In Progress ...\\n\")\n",
    "        self.lang = \"en\"\n",
    "        \n",
    "    def synth(self):\n",
    "        speech = gTTS(text = self.text, lang = self.lang, slow = False)\n",
    "        speech.save(\"output.mp3\")\n",
    "        print(\"Synthesization Completed!\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    s=Synth()\n",
    "    s.synth()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Voice Synthesis Notebook.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
